{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь глосс\n",
    "def load_vocab(vocab_file):\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            token, id_str = parts\n",
    "            token_id = int(id_str)\n",
    "            word2id[token] = token_id\n",
    "            id2word[token_id] = token\n",
    "    return word2id, id2word\n",
    "\n",
    "# из строки глосс в индексы \n",
    "def tokenize_text(text, vocab, add_special_tokens=True):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    tokens = text.strip().split()\n",
    "    token_ids = [vocab.get(tok, vocab.get(\"<unk>\", 0)) for tok in tokens]\n",
    "    if add_special_tokens:\n",
    "        bos = vocab.get(\"<bos>\", 1)\n",
    "        eos = vocab.get(\"<eos>\", 2)\n",
    "        token_ids = [bos] + token_ids + [eos]\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "# токенизатор для кодирования предложения\n",
    "def tokenize_transcript_with_bert(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    encoded = bert_tokenizer.encode(text, add_special_tokens=True)\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, d_model)\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEmbedding(nn.Module):\n",
    "    def __init__(self, cnn_output_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, cnn_output_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(cnn_output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.size()\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        features = self.conv(x)\n",
    "        features = self.pool(features)\n",
    "        features = features.view(B, T, -1)  # (B, T, cnn_output_dim)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageTransformer(nn.Module):\n",
    "    def __init__(self, cnn_output_dim=512, d_model=512, num_encoder_layers=3, num_decoder_layers=3,\n",
    "                 nhead=8, gloss_vocab_size=3194, target_vocab_size=3194, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.spatial_embed = SpatialEmbedding(cnn_output_dim)\n",
    "        self.input_linear = nn.Linear(cnn_output_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.gloss_projection = nn.Linear(d_model, gloss_vocab_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.translation_projection = nn.Linear(d_model, target_vocab_size)\n",
    "    \n",
    "    def forward(self, video_frames, target_seq=None):\n",
    "        # video_frames: (B, T, C, H, W)\n",
    "        B = video_frames.size(0)\n",
    "        spatial_feats = self.spatial_embed(video_frames)     # (B, T, cnn_output_dim)\n",
    "        x = self.input_linear(spatial_feats)                 # (B, T, d_model)\n",
    "        x = self.pos_enc(x)                                  # (B, T, d_model)\n",
    "        encoder_input = x.permute(1, 0, 2)                   # (T, B, d_model)\n",
    "        memory = self.encoder(encoder_input)                 # (T, B, d_model)\n",
    "        memory = memory.permute(1, 0, 2)                       # (B, T, d_model)\n",
    "        gloss_logits = self.gloss_projection(memory)         # (B, T, gloss_vocab_size)\n",
    "        \n",
    "        translation_logits = None\n",
    "        if target_seq is not None:\n",
    "            target_emb = self.word_embedding(target_seq)     # (B, L, d_model)\n",
    "            target_emb = self.pos_enc(target_emb)              # (B, L, d_model)\n",
    "            tgt = target_emb.permute(1, 0, 2)                    # (L, B, d_model)\n",
    "            L = target_seq.size(1)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(video_frames.device)\n",
    "            decoder_output = self.decoder(tgt, memory.permute(1, 0, 2), tgt_mask=tgt_mask)\n",
    "            decoder_output = decoder_output.permute(1, 0, 2)     # (B, L, d_model)\n",
    "            translation_logits = self.translation_projection(decoder_output)  # (B, L, target_vocab_size)\n",
    "        return gloss_logits, translation_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  жадный алгоритм декодинга для перевода \n",
    "def greedy_decode(model, video_frames, max_len, start_symbol):\n",
    "    model.eval()\n",
    "    B = video_frames.size(0)\n",
    "    with torch.no_grad():\n",
    "        spatial_feats = model.spatial_embed(video_frames)\n",
    "        x = model.input_linear(spatial_feats)\n",
    "        x = model.pos_enc(x)\n",
    "        encoder_input = x.permute(1, 0, 2)\n",
    "        memory = model.encoder(encoder_input).permute(1, 0, 2)  # (B, T, d_model)\n",
    "        ys = torch.full((B, 1), start_symbol, dtype=torch.long, device=video_frames.device)\n",
    "        for i in range(max_len - 1):\n",
    "            tgt = model.word_embedding(ys)\n",
    "            tgt = model.pos_enc(tgt)\n",
    "            tgt = tgt.permute(1, 0, 2)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(ys.size(1)).to(video_frames.device)\n",
    "            decoder_output = model.decoder(tgt, memory.permute(1, 0, 2), tgt_mask=tgt_mask)\n",
    "            decoder_output = decoder_output.permute(1, 0, 2)\n",
    "            out = model.translation_projection(decoder_output)\n",
    "            prob = F.log_softmax(out[:, -1, :], dim=-1)\n",
    "            next_word = torch.argmax(prob, dim=-1).unsqueeze(1)\n",
    "            ys = torch.cat([ys, next_word], dim=1)\n",
    "        return ys\n",
    "\n",
    "# Beam search декодинг для перевода \n",
    "def beam_search_decode_debug(model, video_frames, max_len, start_symbol, beam_size=5, end_symbol=None, debug=False):\n",
    "\n",
    "    temperature = 1.5\n",
    "    model.eval()\n",
    "    B = video_frames.size(0)\n",
    "    all_best = []  # храним лучшую последовательность\n",
    "    with torch.no_grad():\n",
    "        spatial_feats = model.spatial_embed(video_frames)  # (B, T, cnn_output_dim)\n",
    "        x = model.input_linear(spatial_feats)              # (B, T, d_model)\n",
    "        x = model.pos_enc(x)                               # (B, T, d_model)\n",
    "        encoder_input = x.permute(1, 0, 2)                 # (T, B, d_model)\n",
    "        memory = model.encoder(encoder_input).permute(1, 0, 2)  # (B, T, d_model)\n",
    "        \n",
    "        for b in range(B):\n",
    "            mem = memory[b:b+1]  # (1, T, d_model)\n",
    "            mem_t = mem.permute(1, 0, 2)  # (T, 1, d_model)\n",
    "            beams = [([start_symbol], 0.0)]\n",
    "            if debug:\n",
    "                print(f\"Sample {b}: initial beam: {beams}\")\n",
    "            for t in range(max_len - 1):\n",
    "                new_beams = []\n",
    "                for seq, score in beams:\n",
    "                    if end_symbol is not None and seq[-1] == end_symbol:\n",
    "                        new_beams.append((seq, score))\n",
    "                        continue\n",
    "                    seq_tensor = torch.tensor(seq, dtype=torch.long, device=video_frames.device).unsqueeze(1)\n",
    "                    tgt = model.word_embedding(seq_tensor)  # (L, 1, d_model)\n",
    "                    tgt = model.pos_enc(tgt)\n",
    "                    tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_tensor.size(0)).to(video_frames.device)\n",
    "                    decoder_output = model.decoder(tgt, mem_t, tgt_mask=tgt_mask)  # (L, 1, d_model)\n",
    "                    decoder_last = decoder_output[-1, 0, :]  # (d_model)\n",
    "                    logits = model.translation_projection(decoder_last)  # (target_vocab_size)\n",
    "                    # log_probs = F.log_softmax(logits, dim=-1)\n",
    "                    log_probs = F.log_softmax(logits / temperature, dim=-1)  \n",
    "                    log_probs[101] = float('-inf')\n",
    "\n",
    "                    top_log_probs, top_indices = torch.topk(log_probs, beam_size)\n",
    "                    top_log_probs = top_log_probs.cpu().numpy()\n",
    "                    top_indices = top_indices.cpu().numpy()\n",
    "                    for i in range(beam_size):\n",
    "                        new_seq = seq + [int(top_indices[i])]\n",
    "                        new_score = score + float(top_log_probs[i])\n",
    "                        new_beams.append((new_seq, new_score))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "                if debug:\n",
    "                    print(f\"Time step {t+1}: beams: {beams}\")\n",
    "            best_seq, best_score = beams[0]\n",
    "            if debug:\n",
    "                print(f\"Best sequence for sample {b}: {best_seq} with score {best_score}\")\n",
    "            all_best.append(best_seq)\n",
    "    \n",
    "    max_length = max(len(seq) for seq in all_best)\n",
    "    decoded = []\n",
    "    for seq in all_best:\n",
    "        padded_seq = seq + [0]*(max_length - len(seq))\n",
    "        decoded.append(padded_seq)\n",
    "    decoded = torch.tensor(decoded, dtype=torch.long, device=video_frames.device)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка фреймов видео + обработка \n",
    "def load_video_frames(video_path, num_frames=50, resize=(224,224)):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"VideoCapture failed to open the file: {video_path}\")\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, resize)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(f\"Failed to load frames from {video_path}\")\n",
    "        \n",
    "    T = len(frames)\n",
    "    if T > num_frames:\n",
    "        indices = np.linspace(0, T - 1, num_frames).astype(int)\n",
    "        frames = [frames[i] for i in indices]\n",
    "    elif T < num_frames:\n",
    "        while len(frames) < num_frames:\n",
    "            frames.append(frames[-1])\n",
    "    frames = np.stack(frames, axis=0)  # (T, H, W, C)\n",
    "    frames = torch.tensor(frames, dtype=torch.float32).permute(0, 3, 1, 2)  # (T, C, H, W)\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, csv_file, video_dir, gloss_vocab, transcript_tokenizer):\n",
    "\n",
    "        self.video_dir = video_dir\n",
    "        self.gloss_vocab = gloss_vocab\n",
    "        self.transcript_tokenizer = transcript_tokenizer\n",
    "        df = pd.read_csv(csv_file)\n",
    "        valid_indices = []\n",
    "        for idx, row in df.iterrows():\n",
    "            video_name = row['video_name']\n",
    "            \n",
    "            video_file = video_name if video_name.endswith(\".mp4\") else video_name + \".mp4\"\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            if os.path.exists(video_path):\n",
    "                valid_indices.append(idx)\n",
    "            else:\n",
    "                print(f\"Warning: Video file not found for row {idx}: {video_path}\")\n",
    "        self.df = df.loc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_name = row['video_name']\n",
    "        video_file = video_name if video_name.endswith(\".mp4\") else video_name + \".mp4\"\n",
    "        video_path = os.path.join(self.video_dir, video_file)\n",
    "        video_tensor = load_video_frames(video_path)  # (T, C, H, W)\n",
    "        gloss_target = tokenize_text(row['glosses'], self.gloss_vocab, add_special_tokens=False)\n",
    "        transcript_target = self.transcript_tokenizer(row['transcript'])\n",
    "        return video_tensor, gloss_target, transcript_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    videos, glosses, transcripts = zip(*batch)\n",
    "    videos = torch.stack(videos, dim=0)  # (B, T, C, H, W)\n",
    "\n",
    "    def pad_sequences(sequences, pad_value=0):\n",
    "        lengths = [seq.size(0) for seq in sequences]\n",
    "        max_len = max(lengths)\n",
    "        padded = torch.full((len(sequences), max_len), pad_value, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded[i, :seq.size(0)] = seq\n",
    "        return padded\n",
    "\n",
    "    padded_glosses = pad_sequences(glosses, pad_value=0)\n",
    "    padded_transcripts = pad_sequences(transcripts, pad_value=0)\n",
    "    return videos, padded_glosses, padded_transcripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, device, checkpoint_dir=\"checkpoints\"):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    translation_loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    lambda_R = 1.0  # вес функцим потерь для распознавания \n",
    "    lambda_T = 1.0  # вес функции потерь для перевода\n",
    "    best_bleu = 0.0\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_references = []\n",
    "        train_hypotheses = []\n",
    "        for videos, glosses, transcripts in tqdm(train_loader):\n",
    "            videos = videos.to(device)\n",
    "            transcripts = transcripts.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            gloss_logits, translation_logits = model(videos, target_seq=transcripts)\n",
    "            # print(gloss_logits)\n",
    "            # print(translation_logits)\n",
    "            gloss_log_probs = F.log_softmax(gloss_logits, dim=-1).permute(1, 0, 2)\n",
    "            B, T, _ = gloss_logits.size()\n",
    "            input_lengths = torch.full((B,), T, dtype=torch.long, device=device)\n",
    "            target_gloss = glosses.view(-1)\n",
    "            target_lengths = torch.full((B,), glosses.size(1), dtype=torch.long, device=device)\n",
    "            loss_ctc = ctc_loss_fn(gloss_log_probs, target_gloss, input_lengths, target_lengths)\n",
    "            loss_trans = translation_loss_fn(translation_logits.view(-1, translation_logits.size(-1)),\n",
    "                                             transcripts.view(-1))\n",
    "            # print(loss_ctc)\n",
    "            # print(loss_trans)\n",
    "            loss = lambda_R * loss_ctc + lambda_T * loss_trans\n",
    "            # loss = lambda_T * loss_trans\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                decoded = beam_search_decode_debug(model, videos, max_len=transcripts.size(1),\n",
    "                                   start_symbol=bert_tokenizer.cls_token_id,\n",
    "                                   beam_size=5, end_symbol=bert_tokenizer.sep_token_id, debug=False)\n",
    "                for i in range(decoded.size(0)):\n",
    "                    pred_tokens = decoded[i].tolist()\n",
    "                    ref_tokens = transcripts[i].tolist()\n",
    "                    pred_sentence = bert_tokenizer.decode([tok for tok in pred_tokens if tok not in [0, bert_tokenizer.cls_token_id, bert_tokenizer.sep_token_id]])\n",
    "                    ref_sentence = bert_tokenizer.decode([tok for tok in ref_tokens if tok not in [0, bert_tokenizer.cls_token_id, bert_tokenizer.sep_token_id]])\n",
    "                    train_hypotheses.append(pred_sentence.split())\n",
    "                    train_references.append([ref_sentence.split()])\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}: Training Loss = {avg_loss:.4f}\")\n",
    "        \n",
    "        train_bleu_score = corpus_bleu(train_references, train_hypotheses)\n",
    "        print(f\"Epoch {epoch}: Training BLEU = {train_bleu_score:.4f}\")\n",
    "        \n",
    "        # валидация: декодинг и подсчет метрики \n",
    "        model.eval()\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "        with torch.no_grad():\n",
    "            for videos, glosses, transcripts in tqdm(val_loader):\n",
    "                videos = videos.to(device)\n",
    "                transcripts = transcripts.to(device)\n",
    "                # decoded = greedy_decode(model, videos, max_len=transcripts.size(1), start_symbol=1)\n",
    "                decoded = beam_search_decode_debug(model, videos, max_len=transcripts.size(1),\n",
    "                                   start_symbol=bert_tokenizer.cls_token_id,\n",
    "                                   beam_size=5, end_symbol=bert_tokenizer.sep_token_id, debug=False)\n",
    "                for i in range(decoded.size(0)):\n",
    "                    pred_tokens = decoded[i].tolist()\n",
    "                    # print(\"pred_tokens\", pred_tokens)\n",
    "                    ref_tokens = transcripts[i].tolist()\n",
    "                    # print(\"ref_tokens\", ref_tokens)\n",
    "                    pred_sentence = bert_tokenizer.decode([tok for tok in pred_tokens if tok not in [0, bert_tokenizer.cls_token_id, bert_tokenizer.sep_token_id]])\n",
    "                    ref_sentence = bert_tokenizer.decode([tok for tok in ref_tokens if tok not in [0, bert_tokenizer.cls_token_id, bert_tokenizer.sep_token_id]])\n",
    "                    # print(\"pred_sentence\", pred_sentence)\n",
    "                    # print(\"ref_sentence\", ref_sentence)\n",
    "                    hypotheses.append(pred_sentence.split())\n",
    "                    references.append([ref_sentence.split()])\n",
    "        bleu_score = corpus_bleu(references, hypotheses)\n",
    "        print(f\"Epoch {epoch}: Validation BLEU = {bleu_score:.4f}\")\n",
    "        if bleu_score > best_bleu:\n",
    "            best_bleu = bleu_score\n",
    "            ckpt_path = os.path.join(checkpoint_dir, f\"model_epoch{epoch}_BLEU{bleu_score:.4f}.pt\")\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"Checkpoint saved: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"./data/train.csv\"                # таблица с данными\n",
    "video_dir = \"./data/video_segments\"          # папка с видео\n",
    "vocab_file = \"./data/vocab.txt\"                   # словарь\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "word2id, id2word = load_vocab(vocab_file)\n",
    "gloss_vocab = word2id\n",
    "gloss_vocab_size = max(gloss_vocab.values()) + 1\n",
    "target_vocab_size = bert_tokenizer.vocab_size\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "train_df = df[df['is_train'] == True][:100].reset_index(drop=True)\n",
    "test_df = df[df['is_train'] == False][:10].reset_index(drop=True)\n",
    "\n",
    "train_csv = \"./data/train_table.csv\"\n",
    "test_csv = \"./data/test_table.csv\"\n",
    "train_df.to_csv(train_csv, index=False)\n",
    "test_df.to_csv(test_csv, index=False)\n",
    "    \n",
    "\n",
    "train_dataset = SignLanguageDataset(train_csv, video_dir, gloss_vocab, tokenize_transcript_with_bert)\n",
    "test_dataset = SignLanguageDataset(test_csv, video_dir, gloss_vocab, tokenize_transcript_with_bert)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SignLanguageTransformer(\n",
    "    cnn_output_dim=512,\n",
    "    d_model=256,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    nhead=4,\n",
    "    gloss_vocab_size=gloss_vocab_size,\n",
    "    target_vocab_size=target_vocab_size,\n",
    "    dropout=0.4)\n",
    "train_model(model, train_loader, test_loader, 15, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
